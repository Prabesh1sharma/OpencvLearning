{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c49561b-bb65-402c-99e5-ea56d5ece1bf",
   "metadata": {},
   "source": [
    "# Image pyramid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018fa5ec-d766-4daf-bf55-40eda49115f6",
   "metadata": {},
   "source": [
    "Work with images with deafault resolution but many times we need to change the resolution (lower it) or resizes the original image in that case image pyramids comes handy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d6bd303b-8eed-4892-ad30-ea4fec874698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d18d02e5-b46c-4cd4-8c1c-ca09979d82db",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"Images/101654506_8eb26cfb60.jpg\")\n",
    "cv2.imshow(\"opencv\",img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a83dc9ad-36ec-4461-abc5-83678f0c1a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(429, 500, 3)\n",
      "(215, 250, 3)\n",
      "(108, 125, 3)\n"
     ]
    }
   ],
   "source": [
    "img = cv2.imread(\"Images/101654506_8eb26cfb60.jpg\")\n",
    "print(img.shape)\n",
    "new = cv2.pyrDown(img)\n",
    "new1 = cv2.pyrDown(new)\n",
    "cv2.imshow(\"opencv\",img)\n",
    "cv2.imshow(\"opencvnew\",new)\n",
    "cv2.imshow(\"opencvnew1\",new1)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "print(new.shape)\n",
    "print(new1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87ed2db9-d0a3-471d-a982-1199a159fbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(108, 125, 3)\n",
      "(216, 250, 3)\n",
      "(432, 500, 3)\n"
     ]
    }
   ],
   "source": [
    "img = cv2.imread(\"Images/101654506_8eb26cfb60.jpg\")\n",
    "\n",
    "big = cv2.pyrUp(new1)\n",
    "big1 = cv2.pyrUp(big)\n",
    "cv2.imshow(\"opencvnew1\",new1)\n",
    "cv2.imshow(\"opencvnewbig\",big)\n",
    "cv2.imshow(\"opencvnewbig1\",big1)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(new1.shape)\n",
    "print(big.shape)\n",
    "print(big1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44894b62-9a22-472b-b1da-cd80cd281740",
   "metadata": {},
   "source": [
    "# Image Translation using opencv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb2cc40-7252-4f72-b6cd-e642b76c5bc8",
   "metadata": {},
   "source": [
    "- hidding a part of the image\n",
    "- Cropping an image\n",
    "- shifting an image\n",
    "- Animating an image using image Translations in loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efdd06b1-22d6-4961-8cf6-0e2ac0437bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"Images/101654506_8eb26cfb60.jpg\")\n",
    "img = cv2.resize(img,(500,500))\n",
    "m = np.float32([[1,0,100],[0,1,50]])\n",
    "new = cv2.warpAffine(img,m,(500,500))\n",
    "cv2.imshow(\"opencv\",img)\n",
    "cv2.imshow(\"opencvnew\",new)\n",
    "cv2.imshow(\"opencvnew1\",m)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f9f064-1a90-4a39-b636-46985b08646e",
   "metadata": {},
   "source": [
    "# Geometric Transformation of images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81453a0-e367-45d3-9f0f-dce9deaa0a2a",
   "metadata": {},
   "source": [
    "- Scaling\n",
    "- Translation\n",
    "- Rotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc81f0d5-160b-4ffa-959a-64e49e6174ac",
   "metadata": {},
   "source": [
    "#### Affine Transformation: \n",
    "All parallel lines in the original image will still be in the output image. To find the Transfromation, matrix we need three points from the input image and their corresponding locations in the output image. The cv.getAffine Transform will create a 2 * 3 matriz which is to be passed to cv.warpAffine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d34af9d-db5e-40ff-b0ad-2d852b6b7848",
   "metadata": {},
   "source": [
    "#### Perspective Transformation \n",
    "We need 3 * 3 transformation matrix. Straight lines will remain straight even after transformation. We need 4 points on the input image and corresponding points on the output image. Among these 4 points, 3 of them should not be collinear. Then the transformation matrix can be found by the function cv.getPerspective Transform. Then apply cv.warpPerspective with this 3*3 transformation matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef72dbab-abac-4c1f-9a15-1116f038464a",
   "metadata": {},
   "source": [
    "# Background Subtraction using Opencv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927ccca9-2b2c-4a19-9ac9-2fb3cfda4ef9",
   "metadata": {},
   "source": [
    "- Its output is a binary segmented image essentially gives information about the non-stationary objects in the image.\n",
    "- There lies a problem in this concept of finding non-stationary portion, as the shadow of the moving object can be moving and sometimes being classified in the foreground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbf86830-e6fa-48a3-a3a0-266f2270598c",
   "metadata": {},
   "outputs": [],
   "source": [
    "org_v = cv2.VideoCapture('Images/448411157_26511716985085856_5565055629309195774_n.mp4')\n",
    "sub_m = cv2.createBackgroundSubtractorMOG2() #its is the masking which removing background of non static object\n",
    "while org_v.isOpened():\n",
    "    r,frame = org_v.read() #r return boolen value if video is playing or not\n",
    "    if r == True:\n",
    "        frame = cv2.resize(frame,(600,400))\n",
    "        sub_v = sub_m.apply(frame)\n",
    "        cv2.imshow(\"opencv\",frame)\n",
    "        cv2.imshow(\"opencv1\",sub_v)\n",
    "        # 0 in the waitkey show the first static frame\n",
    "        if cv2.waitKey(25) & 0xff == ord('p'): #press p to close the video\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "org_v.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78cb654-34ce-4e49-8de3-b8cb3af8f4fc",
   "metadata": {},
   "source": [
    "# Extract images from videos using opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1eaf22fc-500f-4b66-b8bd-ab766dc54946",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('Images/448411157_26511716985085856_5565055629309195774_n.mp4')\n",
    "c=0\n",
    "while cap.isOpened():\n",
    "    r,frame = cap.read() #r return boolen value if video is playing or not\n",
    "    if r == True:\n",
    "        frame = cv2.resize(frame,(600,400))\n",
    "        filename = \"new-folder/org_img\"+str(c) + \".png\"\n",
    "        cv2.imwrite(filename,frame)\n",
    "        cv2.imshow(\"opencv\",frame)\n",
    "        c=c+1\n",
    "        # 0 in the waitkey show the first static frame\n",
    "        if cv2.waitKey(25) & 0xff == ord('p'): #press p to close the video\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308a93e1-4278-446b-a339-a00f8a7903ea",
   "metadata": {},
   "source": [
    "# CVTcolor Method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b61fbd5-ed3c-4f7c-bc1b-30409e60e2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "org = cv2.imread(\"Images/101654506_8eb26cfb60.jpg\")\n",
    "new = cv2.cvtColor(org, cv2.COLOR_BGR2HSV)\n",
    "cv2.imshow(\"org\",org)\n",
    "cv2.imshow(\"gray\",new)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b87460-086e-488e-a6f2-93b22d43753e",
   "metadata": {},
   "source": [
    "# Crop image using Opencv Python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b85972d4-d585-488b-a682-5786c9cbb39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(429, 500, 3)\n"
     ]
    }
   ],
   "source": [
    "img = cv2.imread(\"Images/101654506_8eb26cfb60.jpg\")\n",
    "print(img.shape)\n",
    "img = cv2.resize(img,(500,500))\n",
    "#img[y1:y2, x1:X2]\n",
    "crop = img[195:230,350:460]\n",
    "cv2.imshow(\"img\",img)\n",
    "cv2.imshow(\"crop\",crop)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1954e33e-80b2-48ed-ac26-db427538f4f0",
   "metadata": {},
   "source": [
    "# Create blank image using Opencv Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ce2261b9-fdf6-48f1-9b2c-822db6a3a835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 500, 3)\n"
     ]
    }
   ],
   "source": [
    "#white blank image\n",
    "new_img = np.ones((500,500,3),np.uint8)*255\n",
    "print(new_img.shape)\n",
    "cv2.imshow(\"new_img\",new_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ad36457a-2794-40c1-a9c0-a68c096b00f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 500, 3)\n"
     ]
    }
   ],
   "source": [
    "#black image\n",
    "new_img = np.zeros((500,500,3),np.uint8)*255\n",
    "print(new_img.shape)\n",
    "cv2.imshow(\"new_img\",new_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee039bec-b6fa-4a13-baec-ebd3f085ad08",
   "metadata": {},
   "source": [
    "# Color Pickeer Using Opencv Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef815db-cc34-4f1b-acbe-d123de602c50",
   "metadata": {},
   "source": [
    "  CreateTRacker lines create three trackbars in the \"colour\" window. Each trackbar ranges from 0 to 255 and represents the Red, Green, and Blue color channels respectively. The hancy function is a placeholder for the callback function to be executed when the trackbar position changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "24a06fb0-3ac6-467c-9b09-74ad007f0701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hancy(x):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "adc02405-a988-4cbc-8a1a-2a2ca896196e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.zeros((500,500,3),np.uint8)*255\n",
    "cv2.namedWindow(\"colour\") #creates window \n",
    "cv2.createTrackbar(\"R\",\"colour\",0,255,hancy)\n",
    "cv2.createTrackbar(\"G\",\"colour\",0,255,hancy)\n",
    "cv2.createTrackbar(\"B\",\"colour\",0,255,hancy)\n",
    "while True:\n",
    "    cv2.imshow(\"colour\",img)\n",
    "    if cv2.waitKey(1) & 0xff == ord(\"p\"):\n",
    "        break\n",
    "    r = cv2.getTrackbarPos(\"R\",\"colour\")\n",
    "    g = cv2.getTrackbarPos(\"G\",\"colour\")\n",
    "    b = cv2.getTrackbarPos(\"B\",\"colour\")\n",
    "    img[:] = [b,g,r]\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9371f3-d669-4a4e-ad19-6f3ae71c4868",
   "metadata": {},
   "source": [
    "# getTrackbarPos() function in opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0e0614bf-4ae5-45a9-8d25-55323241c4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.zeros((500,500,3),np.uint8)*255\n",
    "cv2.namedWindow(\"bar\") #creates window \n",
    "cv2.createTrackbar(\"on\",\"bar\",0,100,hancy)\n",
    "while True:\n",
    "    cv2.imshow(\"bar\",img)\n",
    "    if cv2.waitKey(1) & 0xff == ord(\"p\"):\n",
    "        break\n",
    "    var = cv2.getTrackbarPos(\"on\",\"bar\")\n",
    "    img[:] = [var,0,0]\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc80b8a-61ae-4e5f-b7a7-a101c5815dcd",
   "metadata": {},
   "source": [
    "# Region of interest using opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ffa18887-fd37-4764-bb91-4110b76fa920",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"Images/101654506_8eb26cfb60.jpg\")\n",
    "crop = img[130:230,360:430]\n",
    "img[130:230,430:570] = crop #right\n",
    "img[130:230,290:360] = crop #left\n",
    "cv2.imshow(\"opencv\",img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c51c545-9706-4a66-92ca-4078f2c0c741",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
